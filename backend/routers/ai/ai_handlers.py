"""AI ÏßàÏùòÏùëÎãµ Ìï∏Îì§Îü¨ (ai.pyÏóêÏÑú Î∂ÑÎ¶¨)

Ïª®ÌÖçÏä§Ìä∏ Ï§ÄÎπÑ, ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±, ÎãµÎ≥Ä ÏÉùÏÑ±, ÌõÑÏ≤òÎ¶¨, Ïä§Ìä∏Î¶¨Î∞ç Îì±Ïùò ÎπÑÏ¶àÎãàÏä§ Î°úÏßÅ.
"""
import json
import logging
import re
from typing import Optional, AsyncGenerator
from sqlalchemy.orm import Session

from backend.services.search.search_service import get_search_service
from backend.services.search.hybrid_search import get_hybrid_search_service
from backend.services.search.reranker import get_reranker
from backend.services.search.multi_hop_rag import get_multi_hop_rag
from backend.services.ai.ollama_client import ollama_generate, ollama_generate_stream
from backend.services.ai.context_manager import get_context_manager

logger = logging.getLogger(__name__)

# Ïú†ÏÇ¨ÎèÑ ÏûÑÍ≥ÑÍ∞í (0.3 ÎØ∏ÎßåÏù¥Î©¥ Í¥ÄÎ†®ÏÑ± ÎÇÆÏùå)
SIMILARITY_THRESHOLD = 0.3

AI_SYSTEM_PROMPT = """ÎãπÏã†ÏùÄ ÌïúÍµ≠Ïñ¥ Ï†ÑÏö© AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. Îã§Ïùå Í∑úÏπôÏùÑ Î∞òÎìúÏãú ÏßÄÌÇ§ÏÑ∏Ïöî:
- Î∞òÎìúÏãú ÌïúÍµ≠Ïñ¥Î°úÎßå ÎãµÎ≥ÄÌïòÏÑ∏Ïöî. ÏòÅÏñ¥¬∑Ï§ëÍµ≠Ïñ¥(‰∏≠Êñá)¬∑ÏùºÎ≥∏Ïñ¥Î°ú ÎãµÎ≥ÄÌïòÏßÄ ÎßàÏÑ∏Ïöî.
- Ïª®ÌÖçÏä§Ìä∏Ïóê ÏóÜÎäî ÎÇ¥Ïö©ÏùÄ Ï∂îÏ∏°ÌïòÏßÄ ÎßàÏÑ∏Ïöî.
- Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Í∞ÑÍ≤∞ÌïòÍ≤å ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.
- Î∂àÌïÑÏöîÌïú Î∞òÎ≥µ, Ïù¥Î™®ÏßÄ, Ïû•ÏãùÏ†Å ÌëúÌòÑÏùÑ ÌîºÌïòÏÑ∏Ïöî.
- ÏòÅÏñ¥ Î¨∏Ïû•, ÏòÅÏñ¥ ÏÑ§Î™Ö, ÏΩîÎìú Î∏îÎ°ùÏùÑ Ìè¨Ìï®ÌïòÏßÄ ÎßàÏÑ∏Ïöî."""


# ---------------------------------------------------------------------------
# Ïª®ÌÖçÏä§Ìä∏ Ï§ÄÎπÑ
# ---------------------------------------------------------------------------

def prepare_question_context(request) -> tuple:
    """ÏßàÎ¨∏ Ïª®ÌÖçÏä§Ìä∏ Ï§ÄÎπÑ (Í∏∞Î≥∏: ÏùòÎØ∏ Í≤ÄÏÉâÎßå)."""
    search_service = get_search_service()
    context_docs = []
    if request.context_enabled:
        context_docs = search_service.search_simple(
            request.question.strip(), top_k=request.top_k
        )
    MAX_CONTEXT_LENGTH = 1000
    context_text = ""
    sources = []
    current_length = 0
    has_relevant_context = False
    similar_docs = []
    for doc in context_docs:
        score = doc.get("score", 0.0)
        if score < SIMILARITY_THRESHOLD:
            similar_docs.append({"file": doc["file"], "score": score})
            continue
        has_relevant_context = True
        doc_content = doc.get("content", "")
        if len(doc_content) > 300:
            doc_content = doc_content[:297] + "..."
        doc_text = f"[{doc.get('file', '')}]\n{doc_content}\n\n"
        if current_length + len(doc_text) > MAX_CONTEXT_LENGTH:
            remaining = MAX_CONTEXT_LENGTH - current_length
            if remaining > 50:
                doc_text = doc_text[:remaining]
                context_text += doc_text
            break
        context_text += doc_text
        current_length += len(doc_text)
        sources.append({
            "file": doc.get("file", ""),
            "score": doc.get("score", 0),
            "snippet": doc.get("snippet", ""),
        })
    return context_docs, context_text, sources, has_relevant_context, similar_docs


def prepare_question_context_enhanced(request, db: Session) -> tuple:
    """ÏßàÎ¨∏ Ïª®ÌÖçÏä§Ìä∏ Ï§ÄÎπÑ (Phase 9-3-3: Hybrid/Rerank/Multi-hop + ContextManager)."""
    if not request.context_enabled:
        return [], "", [], False, []
    question = request.question.strip()
    top_k = request.top_k
    context_docs: list = []

    if request.use_multihop and db:
        mh = get_multi_hop_rag()
        out = mh.search(db=db, question=question, initial_top_k=top_k)
        context_docs = out.get("chunks", [])
    elif request.search_mode == "hybrid" and db:
        hybrid_svc = get_hybrid_search_service()
        context_docs = hybrid_svc.search_hybrid(
            db=db, query=question, top_k=top_k
        )
    else:
        search_svc = get_search_service()
        result = search_svc.search(query=question, top_k=top_k, offset=0)
        context_docs = result.get("results", [])

    if request.use_reranking and context_docs:
        reranker_svc = get_reranker()
        context_docs = reranker_svc.rerank(
            query=question,
            candidates=context_docs,
            top_k=top_k,
            content_key="content",
        )
        for d in context_docs:
            d["score"] = d.get("final_score", d.get("score", 0))

    ctx_mgr = get_context_manager()
    built = ctx_mgr.build_context(
        question=question,
        search_results=context_docs,
        max_tokens=None,
        content_key="content",
        score_key="score",
    )
    context_text = built.get("context", "")
    chunks_used = built.get("chunks_used", [])
    has_relevant_context = bool(context_text.strip())
    sources = []
    for c in chunks_used:
        sources.append({
            "file": "",
            "score": c.get("relevance_score", 0),
            "snippet": (c.get("content") or "")[:200],
        })
    similar_docs = []
    for doc in context_docs:
        if doc.get("score", 0) < SIMILARITY_THRESHOLD:
            similar_docs.append({
                "file": doc.get("file", ""),
                "score": doc.get("score", 0),
            })
    return context_docs, context_text, sources, has_relevant_context, similar_docs


# ---------------------------------------------------------------------------
# ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ± / ÌõÑÏ≤òÎ¶¨
# ---------------------------------------------------------------------------

def build_prompt(question: str, context_text: str, has_relevant_context: bool, similar_docs: list) -> str:
    """ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ± -- ÏãúÏä§ÌÖú ÏßÄÏãúÎäî AI_SYSTEM_PROMPTÎ°ú Î∂ÑÎ¶¨Îê®."""
    if not context_text or not has_relevant_context:
        similar_docs_text = ""
        if similar_docs:
            similar_docs_text = "\n\nÏ∞∏Í≥†: Îã§Ïùå Î¨∏ÏÑúÎì§Ïù¥ Ïú†ÏÇ¨ÌïòÏßÄÎßå ÏßÅÏ†ëÏ†ÅÏù∏ ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌïòÍ∏∞ÏóêÎäî Í¥ÄÎ†®ÏÑ±Ïù¥ ÎÇÆÏäµÎãàÎã§:\n"
            similar_docs_text += "\n".join([f"- {doc['file']} (Ïú†ÏÇ¨ÎèÑ: {doc['score']*100:.1f}%)" for doc in similar_docs[:3]])

        return f"""ÏßàÎ¨∏: {question}
{similar_docs_text}
ÏßÄÏãù Î≤†Ïù¥Ïä§Ïóê Ï†ïÎ≥¥Í∞Ä ÏóÜÏúºÎ©¥ "ÏßàÎ¨∏ÌïòÏã† ÎÇ¥Ïö©Ïóê ÎåÄÌïú Ï†ïÎ≥¥Í∞Ä ÏßÄÏãù Î≤†Ïù¥Ïä§Ïóê ÏóÜÏäµÎãàÎã§."ÎùºÍ≥†Îßå ÎãµÎ≥ÄÌïòÏÑ∏Ïöî."""
    else:
        return f"""Ïª®ÌÖçÏä§Ìä∏:
{context_text}

ÏßàÎ¨∏: {question}

Ïª®ÌÖçÏä§Ìä∏Ïùò Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°úÎßå ÎãµÎ≥ÄÌïòÏÑ∏Ïöî."""


def postprocess_answer(answer: str) -> str:
    """ÎãµÎ≥Ä ÌõÑÏ≤òÎ¶¨ -- System Prompt Î∂ÑÎ¶¨ ÌõÑ Í≤ΩÎüâÌôî (Phase 13-5-3)."""
    answer = answer.strip()

    # Ïù¥Î™®ÏßÄ ÌÅ¥Îü¨Ïä§ÌÑ∞ Ï†úÍ±∞
    answer = re.sub(r'([üòäü§îüí™üìùüîçü§ùüí≠üéâ]+\s*)+', '', answer)

    # LLMÏù¥ ÌîÑÎ°¨ÌîÑÌä∏Î•º Î∞òÎ≥µÌïòÎäî Í≤ΩÏö∞ Ï†úÍ±∞
    answer = re.sub(r'^(Please|You should|I\'m waiting|Your answer|This code)[\s\S]*?\.\s*', '', answer, flags=re.IGNORECASE)

    # Ïó∞ÏÜçÎêú Îπà Ï§Ñ Ï†ïÎ¶¨
    answer = re.sub(r'\n{3,}', '\n\n', answer)

    return answer.strip()


# ---------------------------------------------------------------------------
# AI ÎãµÎ≥Ä ÏÉùÏÑ±
# ---------------------------------------------------------------------------

def generate_ai_answer(
    question: str,
    context_text: str,
    max_tokens: int,
    temperature: float,
    has_relevant_context: bool = True,
    similar_docs: list = None,
) -> str:
    """AI ÎãµÎ≥Ä ÏÉùÏÑ± (Ollama)"""
    if similar_docs is None:
        similar_docs = []

    prompt = build_prompt(question, context_text, has_relevant_context, similar_docs)
    prompt_length = len(prompt)
    logger.info(f"Ollama ÎãµÎ≥Ä ÏÉùÏÑ± ÏãúÏûë (ÏßàÎ¨∏: {question[:50]}..., ÌîÑÎ°¨ÌîÑÌä∏ Í∏∏Ïù¥: {prompt_length}Ïûê, Í¥ÄÎ†® Ïª®ÌÖçÏä§Ìä∏: {has_relevant_context})")

    if prompt_length > 1600 and has_relevant_context:
        logger.warning(f"ÌîÑÎ°¨ÌîÑÌä∏Í∞Ä Í∏∏Ïñ¥ÏÑú Ïª®ÌÖçÏä§Ìä∏Î•º Ï∂ïÏÜåÌï©ÎãàÎã§ ({prompt_length}Ïûê -> 1600ÏûêÎ°ú Ï†úÌïú)")
        template_length = len(prompt) - len(context_text) - len(question)
        max_context = 1600 - template_length - len(question)
        if max_context > 0 and len(context_text) > max_context:
            context_text = context_text[:max_context] + "..."
            prompt = build_prompt(question, context_text, has_relevant_context, similar_docs)

    answer = ollama_generate(
        prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        top_k=40,
        top_p=0.9,
        repeat_penalty=1.2,
        system_prompt=AI_SYSTEM_PROMPT,
    )
    if answer is None:
        raise ValueError("Ollama ÏùëÎãµ ÏóÜÏùå")
    answer = postprocess_answer(answer)
    logger.info(f"Ollama ÎãµÎ≥Ä ÏÉùÏÑ± ÏôÑÎ£å (Í∏∏Ïù¥: {len(answer)} Î¨∏Ïûê)")

    min_length = 10 if not has_relevant_context else 20
    if not answer or len(answer.strip()) < min_length:
        logger.warning(f"Ollama ÎãµÎ≥ÄÏù¥ ÎÑàÎ¨¥ ÏßßÍ±∞ÎÇò ÎπÑÏñ¥ÏûàÏùå (ÏµúÏÜå Í∏∏Ïù¥: {min_length})")
        if not has_relevant_context:
            return "ÏßàÎ¨∏ÌïòÏã† ÎÇ¥Ïö©Ïóê ÎåÄÌïú Ï†ïÎ≥¥Í∞Ä ÏßÄÏãù Î≤†Ïù¥Ïä§Ïóê ÏóÜÏäµÎãàÎã§."
        raise ValueError("ÏÉùÏÑ±Îêú ÎãµÎ≥ÄÏù¥ ÎÑàÎ¨¥ ÏßßÏäµÎãàÎã§")

    return answer


def generate_fallback_answer(
    context_docs: list,
    has_model: bool,
    has_relevant_context: bool = False,
    similar_docs: list = None,
) -> str:
    """Ìè¥Î∞± ÎãµÎ≥Ä ÏÉùÏÑ±"""
    if similar_docs is None:
        similar_docs = []

    if not has_relevant_context:
        if similar_docs:
            similar_docs_text = "\n\nÏ∞∏Í≥†: Îã§Ïùå Î¨∏ÏÑúÎì§Ïù¥ Ïú†ÏÇ¨ÌïòÏßÄÎßå ÏßÅÏ†ëÏ†ÅÏù∏ ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌïòÍ∏∞ÏóêÎäî Í¥ÄÎ†®ÏÑ±Ïù¥ ÎÇÆÏäµÎãàÎã§:\n"
            similar_docs_text += "\n".join([f"- {doc['file']} (Ïú†ÏÇ¨ÎèÑ: {doc['score']*100:.1f}%)" for doc in similar_docs[:3]])
            return f"ÏßàÎ¨∏ÌïòÏã† ÎÇ¥Ïö©Ïóê ÎåÄÌïú Ï†ïÎ≥¥Í∞Ä ÏßÄÏãù Î≤†Ïù¥Ïä§Ïóê ÏóÜÏäµÎãàÎã§.{similar_docs_text}"
        else:
            return "ÏßàÎ¨∏ÌïòÏã† ÎÇ¥Ïö©Ïóê ÎåÄÌïú Ï†ïÎ≥¥Í∞Ä ÏßÄÏãù Î≤†Ïù¥Ïä§Ïóê ÏóÜÏäµÎãàÎã§."

    if context_docs:
        if has_model:
            return f"Í¥ÄÎ†® Î¨∏ÏÑú {len(context_docs)}Í∞úÎ•º Ï∞æÏïòÏäµÎãàÎã§. ÏúÑÏùò Ïª®ÌÖçÏä§Ìä∏Î•º Ï∞∏Í≥†ÌïòÏÑ∏Ïöî.\n\nÏ∞∏Í≥†: AI Î™®Îç∏ ÏùëÎãµ ÏÉùÏÑ±Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§."
        else:
            return f"""Í¥ÄÎ†® Î¨∏ÏÑú {len(context_docs)}Í∞úÎ•º Ï∞æÏïòÏäµÎãàÎã§.

Ï∞∏Í≥† Î¨∏ÏÑú:
{chr(10).join([f"- {doc['file']} (Ïú†ÏÇ¨ÎèÑ: {doc['score']*100:.1f}%)" for doc in context_docs[:5]])}

ÏúÑÏùò Ïª®ÌÖçÏä§Ìä∏Î•º Ï∞∏Í≥†ÌïòÏó¨ ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.

Ï∞∏Í≥†: AI Î™®Îç∏Ïù¥ ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïÑ Ï∂îÎ°†Ï†Å ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§. OllamaÎ•º Ïã§ÌñâÌïòÍ≥† Î™®Îç∏(eeve-korean Îì±)ÏùÑ Î°úÎìúÌïòÎ©¥ Îçî ÏÉÅÏÑ∏Ìïú ÎãµÎ≥ÄÏùÑ Î∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§."""
    else:
        if has_model:
            return "Í¥ÄÎ†® Î¨∏ÏÑúÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\n\nÏ∞∏Í≥†: AI Î™®Îç∏ ÏùëÎãµ ÏÉùÏÑ±Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§."
        else:
            return "Í¥ÄÎ†® Î¨∏ÏÑúÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\n\nÏ∞∏Í≥†: AI Î™®Îç∏Ïù¥ ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïÑ Ï∂îÎ°†Ï†Å ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§."


# ---------------------------------------------------------------------------
# Ïä§Ìä∏Î¶¨Î∞ç ÎãµÎ≥Ä
# ---------------------------------------------------------------------------

async def generate_streaming_answer(
    question: str,
    context_text: str,
    max_tokens: int,
    temperature: float,
    has_relevant_context: bool = True,
    similar_docs: list = None,
) -> AsyncGenerator[str, None]:
    """True Streaming AI ÎãµÎ≥Ä ÏÉùÏÑ± -- Ollama ÌÜ†ÌÅ∞ Ï¶âÏãú SSE Ï†ÑÎã¨ (Phase 13-5-1)."""
    if similar_docs is None:
        similar_docs = []
    prompt = build_prompt(question, context_text, has_relevant_context, similar_docs)

    try:
        for token in ollama_generate_stream(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_k=40,
            top_p=0.9,
            repeat_penalty=1.2,
            system_prompt=AI_SYSTEM_PROMPT,
        ):
            data = {"type": "chunk", "content": token}
            yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
        data = {"type": "done", "content": ""}
        yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
    except Exception as e:
        error_data = {"type": "error", "content": str(e)}
        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
