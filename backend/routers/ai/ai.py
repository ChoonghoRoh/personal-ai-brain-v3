"""AI API ë¼ìš°í„° â€” Ollama ë¡œì»¬ LLM (EEVE-Korean ë“±) ì‚¬ìš©"""
from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, AsyncGenerator
from sqlalchemy.orm import Session
import logging
import json

from backend.services.search.search_service import get_search_service
from backend.services.search.hybrid_search import get_hybrid_search_service
from backend.services.search.reranker import get_reranker
from backend.services.search.multi_hop_rag import get_multi_hop_rag
from backend.services.ai.ollama_client import ollama_generate, ollama_available, ollama_connection_check
from backend.services.ai.context_manager import get_context_manager
from backend.models.database import get_db

router = APIRouter(prefix="/api/ask", tags=["ai"])

logger = logging.getLogger(__name__)

# ìœ ì‚¬ë„ ì„ê³„ê°’ (0.3 ë¯¸ë§Œì´ë©´ ê´€ë ¨ì„± ë‚®ìŒ)
SIMILARITY_THRESHOLD = 0.3


class AskRequest(BaseModel):
    question: str
    context_enabled: bool = True
    top_k: int = 5
    max_tokens: int = 500  # ê¸°ë³¸ê°’ ì¦ê°€
    temperature: float = 0.7
    # Phase 9-3-3: RAG ê°œì„  ì˜µì…˜ (ê¸°ë³¸ê°’ = ê¸°ì¡´ ë™ì‘ ìœ ì§€)
    search_mode: str = "semantic"  # semantic | hybrid
    use_reranking: bool = False
    use_multihop: bool = False


class AskResponse(BaseModel):
    answer: str
    context: list
    sources: list
    model_used: Optional[str] = None  # ì‚¬ìš©ëœ ëª¨ë¸ ì •ë³´
    error: Optional[str] = None  # ì˜¤ë¥˜ ì •ë³´ (ìˆëŠ” ê²½ìš°)
    ollama_feedback: Optional[dict] = None  # Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ ê²°ê³¼ (available, message, detail) â€” êº¼ì ¸ ìˆìœ¼ë©´ í”¼ë“œë°±ì— í™œìš©


def prepare_question_context(request: AskRequest) -> tuple:
    """ì§ˆë¬¸ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ê¸°ë³¸: ì˜ë¯¸ ê²€ìƒ‰ë§Œ)."""
    search_service = get_search_service()
    context_docs = []
    if request.context_enabled:
        context_docs = search_service.search_simple(
            request.question.strip(), top_k=request.top_k
        )
    MAX_CONTEXT_LENGTH = 1000
    context_text = ""
    sources = []
    current_length = 0
    has_relevant_context = False
    similar_docs = []
    for doc in context_docs:
        score = doc.get("score", 0.0)
        if score < SIMILARITY_THRESHOLD:
            similar_docs.append({"file": doc["file"], "score": score})
            continue
        has_relevant_context = True
        doc_content = doc.get("content", "")
        if len(doc_content) > 300:
            doc_content = doc_content[:297] + "..."
        doc_text = f"[{doc.get('file', '')}]\n{doc_content}\n\n"
        if current_length + len(doc_text) > MAX_CONTEXT_LENGTH:
            remaining = MAX_CONTEXT_LENGTH - current_length
            if remaining > 50:
                doc_text = doc_text[:remaining]
                context_text += doc_text
            break
        context_text += doc_text
        current_length += len(doc_text)
        sources.append({
            "file": doc.get("file", ""),
            "score": doc.get("score", 0),
            "snippet": doc.get("snippet", ""),
        })
    return context_docs, context_text, sources, has_relevant_context, similar_docs


def prepare_question_context_enhanced(request: AskRequest, db: Session) -> tuple:
    """ì§ˆë¬¸ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (Phase 9-3-3: Hybrid/Rerank/Multi-hop + ContextManager)."""
    if not request.context_enabled:
        return [], "", [], False, []
    question = request.question.strip()
    top_k = request.top_k
    context_docs: list = []

    if request.use_multihop and db:
        mh = get_multi_hop_rag()
        out = mh.search(db=db, question=question, initial_top_k=top_k)
        context_docs = out.get("chunks", [])
    elif request.search_mode == "hybrid" and db:
        hybrid_svc = get_hybrid_search_service()
        context_docs = hybrid_svc.search_hybrid(
            db=db, query=question, top_k=top_k
        )
    else:
        search_svc = get_search_service()
        result = search_svc.search(query=question, top_k=top_k, offset=0)
        context_docs = result.get("results", [])

    if request.use_reranking and context_docs:
        reranker_svc = get_reranker()
        context_docs = reranker_svc.rerank(
            query=question,
            candidates=context_docs,
            top_k=top_k,
            content_key="content",
        )
        for d in context_docs:
            d["score"] = d.get("final_score", d.get("score", 0))

    ctx_mgr = get_context_manager()
    built = ctx_mgr.build_context(
        question=question,
        search_results=context_docs,
        max_tokens=None,
        content_key="content",
        score_key="score",
    )
    context_text = built.get("context", "")
    chunks_used = built.get("chunks_used", [])
    has_relevant_context = bool(context_text.strip())
    sources = []
    for c in chunks_used:
        sources.append({
            "file": "",  # chunks_usedì—ëŠ” fileì´ ì—†ì„ ìˆ˜ ìˆìŒ
            "score": c.get("relevance_score", 0),
            "snippet": (c.get("content") or "")[:200],
        })
    similar_docs = []
    for doc in context_docs:
        if doc.get("score", 0) < SIMILARITY_THRESHOLD:
            similar_docs.append({
                "file": doc.get("file", ""),
                "score": doc.get("score", 0),
            })
    return context_docs, context_text, sources, has_relevant_context, similar_docs


def build_prompt(question: str, context_text: str, has_relevant_context: bool, similar_docs: list) -> str:
    """í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
    if not context_text or not has_relevant_context:
        # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ê´€ë ¨ì„±ì´ ë‚®ì€ ê²½ìš°
        similar_docs_text = ""
        if similar_docs:
            similar_docs_text = "\n\nì°¸ê³ : ë‹¤ìŒ ë¬¸ì„œë“¤ì´ ìœ ì‚¬í•˜ì§€ë§Œ ì§ì ‘ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ê¸°ì—ëŠ” ê´€ë ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤:\n"
            similar_docs_text += "\n".join([f"- {doc['file']} (ìœ ì‚¬ë„: {doc['score']*100:.1f}%)" for doc in similar_docs[:3]])
        
        return f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì ˆëŒ€ë¡œ ì˜ì–´ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.

ì§ˆë¬¸: {question}

ì¤‘ìš” ì§€ì‹œì‚¬í•­:
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì˜ì–´Â·ì¤‘êµ­ì–´(ä¸­æ–‡)ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
2. ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ê°€ ì§€ì‹ ë² ì´ìŠ¤ì— ì—†ìœ¼ë©´ "ì§ˆë¬¸í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ì •ë³´ê°€ ì§€ì‹ ë² ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
3. ì¼ë°˜ì ì¸ ì§€ì‹ì´ë‚˜ ì¶”ì¸¡ìœ¼ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
4. ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì ˆëŒ€ ë‹µë³€ì„ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
5. ì˜ì–´ ë¬¸ì¥, ì˜ì–´ ì„¤ëª…, ì˜ì–´ ì½”ë“œ ì£¼ì„ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.{similar_docs_text}

í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:"""
    else:
        # ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°
        return f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì ˆëŒ€ë¡œ ì˜ì–´ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context_text}

ì§ˆë¬¸: {question}

ì¤‘ìš” ì§€ì‹œì‚¬í•­:
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì˜ì–´Â·ì¤‘êµ­ì–´(ä¸­æ–‡)ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
2. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
3. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
4. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
5. ë¶ˆí•„ìš”í•œ ë°˜ë³µ, ì´ëª¨ì§€, ì¥ì‹ì ì¸ í‘œí˜„ì„ í”¼í•˜ì„¸ìš”.
6. ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
7. ì˜ì–´ ë¬¸ì¥, ì˜ì–´ ì„¤ëª…, ì˜ì–´ ì½”ë“œ ì£¼ì„ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:"""


def postprocess_answer(answer: str) -> str:
    """ë‹µë³€ í›„ì²˜ë¦¬"""
    import re
    answer = answer.strip()
    
    # ì½”ë“œ ë¸”ë¡ ì œê±° (```python, ``` ë“±)
    answer = re.sub(r'```[\s\S]*?```', '', answer)
    answer = re.sub(r'`[^`]+`', '', answer)
    
    # ì˜ì–´ ì§€ì‹œì‚¬í•­ íŒ¨í„´ ì œê±°
    answer = re.sub(r'Please respond in Korean[\s\S]*?You should[\s\S]*?\.', '', answer, flags=re.IGNORECASE)
    answer = re.sub(r'Please respond[\s\S]*?\.', '', answer, flags=re.IGNORECASE)
    answer = re.sub(r'I\'m waiting[\s\S]*?\.', '', answer, flags=re.IGNORECASE)
    answer = re.sub(r'You should only[\s\S]*?\.', '', answer, flags=re.IGNORECASE)
    answer = re.sub(r'Your answer should[\s\S]*?\.', '', answer, flags=re.IGNORECASE)
    
    # ì˜ì–´ ë¬¸ì¥ ì œê±° (ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ê³  ë§ˆì¹¨í‘œë¡œ ëë‚˜ëŠ” ì˜ì–´ ë¬¸ì¥)
    # ë‹¨, í•œêµ­ì–´ì™€ ì„ì¸ ê²½ìš°ëŠ” ë³´ì¡´
    lines = answer.split('\n')
    filtered_lines = []
    for line in lines:
        # ì˜ì–´ë§Œ ìˆëŠ” ì¤„ ì œê±° (í•œêµ­ì–´ê°€ í¬í•¨ëœ ì¤„ì€ ë³´ì¡´)
        if re.match(r'^[A-Z][^ê°€-í£]*[.!?]\s*$', line.strip()) and not re.search(r'[ê°€-í£]', line):
            continue
        # "Please", "You should", "I'm waiting" ë“±ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì œê±°
        if re.match(r'^(Please|You should|I\'m waiting|Your answer)', line.strip(), re.IGNORECASE):
            continue
        filtered_lines.append(line)
    answer = '\n'.join(filtered_lines)
    
    # ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°
    answer = re.sub(r'\(í† í° ì œí•œ ê³ ë ¤í•˜ì—¬[^)]*\)\s*ğŸ’ª', '', answer)
    answer = re.sub(r'ğŸ’ª\s*$', '', answer)
    answer = re.sub(r'\.\.\.\s*\(ê³¼ì •ì„ ì¬í˜„\)\s*ğŸ’­', '', answer)
    answer = re.sub(r'([ğŸ˜ŠğŸ¤”ğŸ’ªğŸ“ğŸ”ğŸ¤ğŸ’­ğŸ‰]+\s*)+', '', answer)
    
    # "This code", "This function" ê°™ì€ ì˜ì–´ ì„¤ëª… ì œê±°
    answer = re.sub(r'This (code|function|method|class)[\s\S]*?\.', '', answer, flags=re.IGNORECASE)
    answer = re.sub(r'This defines[\s\S]*?\.', '', answer, flags=re.IGNORECASE)
    
    # "import"ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì œê±° (ì½”ë“œ ì˜ˆì œ)
    lines = answer.split('\n')
    filtered_lines = []
    for line in lines:
        if not line.strip().startswith('import ') and not line.strip().startswith('def ') and not line.strip().startswith('class '):
            filtered_lines.append(line)
    answer = '\n'.join(filtered_lines)
    
    # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
    answer = re.sub(r'\n{3,}', '\n\n', answer)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    answer = answer.strip()
    
    # ì˜ì–´ë§Œ ìˆëŠ” ë¬¸ë‹¨ ì œê±° (í•œêµ­ì–´ê°€ ì „í˜€ ì—†ëŠ” ê²½ìš°)
    paragraphs = answer.split('\n\n')
    filtered_paragraphs = []
    for para in paragraphs:
        if re.search(r'[ê°€-í£]', para):  # í•œêµ­ì–´ê°€ í¬í•¨ëœ ë¬¸ë‹¨ë§Œ ë³´ì¡´
            filtered_paragraphs.append(para)
        elif not re.match(r'^[A-Z][^ê°€-í£]*[.!?]\s*$', para.strip()):  # ì˜ì–´ ë¬¸ì¥ì´ ì•„ë‹Œ ê²½ìš°ë„ ë³´ì¡´
            filtered_paragraphs.append(para)
    answer = '\n\n'.join(filtered_paragraphs)
    
    return answer.strip()


def generate_ai_answer(
    question: str,
    context_text: str,
    max_tokens: int,
    temperature: float,
    has_relevant_context: bool = True,
    similar_docs: list = None
) -> str:
    """AI ë‹µë³€ ìƒì„± (Ollama)"""
    if similar_docs is None:
        similar_docs = []
    
    prompt = build_prompt(question, context_text, has_relevant_context, similar_docs)
    prompt_length = len(prompt)
    logger.info(f"Ollama ë‹µë³€ ìƒì„± ì‹œì‘ (ì§ˆë¬¸: {question[:50]}..., í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {prompt_length}ì, ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸: {has_relevant_context})")
    
    # í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ê²½ê³  ë° ìë™ ì¶•ì†Œ
    if prompt_length > 1600 and has_relevant_context:
        logger.warning(f"í”„ë¡¬í”„íŠ¸ê°€ ê¸¸ì–´ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶•ì†Œí•©ë‹ˆë‹¤ ({prompt_length}ì -> 1600ìë¡œ ì œí•œ)")
        template_length = len(prompt) - len(context_text) - len(question)
        max_context = 1600 - template_length - len(question)
        if max_context > 0 and len(context_text) > max_context:
            context_text = context_text[:max_context] + "..."
            prompt = build_prompt(question, context_text, has_relevant_context, similar_docs)
    
    answer = ollama_generate(
        prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        top_k=40,
        top_p=0.9,
        repeat_penalty=1.2,
    )
    if answer is None:
        raise ValueError("Ollama ì‘ë‹µ ì—†ìŒ")
    answer = postprocess_answer(answer)
    logger.info(f"Ollama ë‹µë³€ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(answer)} ë¬¸ì)")
    
    # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ì„ ë•ŒëŠ” ì§§ì€ ë‹µë³€ë„ í—ˆìš©
    min_length = 10 if not has_relevant_context else 20
    if not answer or len(answer.strip()) < min_length:
        logger.warning(f"Ollama ë‹µë³€ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŒ (ìµœì†Œ ê¸¸ì´: {min_length})")
        if not has_relevant_context:
            return "ì§ˆë¬¸í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ì •ë³´ê°€ ì§€ì‹ ë² ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤."
        raise ValueError("ìƒì„±ëœ ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")
    
    return answer


def generate_fallback_answer(context_docs: list, has_model: bool, has_relevant_context: bool = False, similar_docs: list = None) -> str:
    """í´ë°± ë‹µë³€ ìƒì„±"""
    if similar_docs is None:
        similar_docs = []
    
    if not has_relevant_context:
        # ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°
        if similar_docs:
            similar_docs_text = "\n\nì°¸ê³ : ë‹¤ìŒ ë¬¸ì„œë“¤ì´ ìœ ì‚¬í•˜ì§€ë§Œ ì§ì ‘ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ê¸°ì—ëŠ” ê´€ë ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤:\n"
            similar_docs_text += "\n".join([f"- {doc['file']} (ìœ ì‚¬ë„: {doc['score']*100:.1f}%)" for doc in similar_docs[:3]])
            return f"ì§ˆë¬¸í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ì •ë³´ê°€ ì§€ì‹ ë² ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤.{similar_docs_text}"
        else:
            return "ì§ˆë¬¸í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ì •ë³´ê°€ ì§€ì‹ ë² ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤."
    
    # ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°
    if context_docs:
        if has_model:
            return f"ê´€ë ¨ ë¬¸ì„œ {len(context_docs)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ìœ„ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n\nì°¸ê³ : AI ëª¨ë¸ ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        else:
            return f"""ê´€ë ¨ ë¬¸ì„œ {len(context_docs)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ:
{chr(10).join([f"- {doc['file']} (ìœ ì‚¬ë„: {doc['score']*100:.1f}%)" for doc in context_docs[:5]])}

ìœ„ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ì°¸ê³ : AI ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì¶”ë¡ ì  ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollamaë¥¼ ì‹¤í–‰í•˜ê³  ëª¨ë¸(eeve-korean ë“±)ì„ ë¡œë“œí•˜ë©´ ë” ìƒì„¸í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
    else:
        if has_model:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nì°¸ê³ : AI ëª¨ë¸ ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        else:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nì°¸ê³ : AI ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì¶”ë¡ ì  ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


@router.post(
    "",
    summary="AI ì§ˆì˜ ì‘ë‹µ",
    description="ì§€ì‹ë² ì´ìŠ¤ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ AI ì§ˆì˜ì‘ë‹µ. search_mode(semantic|hybrid), use_reranking, use_multihop ì˜µì…˜ ì§€ì›.",
    responses={
        200: {"description": "ì„±ê³µ (AskResponse)"},
        400: {"description": "ì§ˆë¬¸ ëˆ„ë½ ë˜ëŠ” ì˜ëª»ëœ ìš”ì²­"},
        500: {"description": "AI ì‘ë‹µ ìƒì„± ì˜¤ë¥˜"},
    },
)
async def ask_question(
    request: AskRequest,
    db: Session = Depends(get_db),
) -> AskResponse:
    """AI ì§ˆì˜ ì‘ë‹µ (Phase 9-3-3: search_mode, use_reranking, use_multihop ì§€ì›)."""
    if not request.question or not request.question.strip():
        raise HTTPException(status_code=400, detail="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

    try:
        use_enhanced = (
            request.search_mode == "hybrid"
            or request.use_reranking
            or request.use_multihop
        )
        if use_enhanced:
            context_docs, context_text, sources, has_relevant_context, similar_docs = (
                prepare_question_context_enhanced(request, db)
            )
        else:
            context_docs, context_text, sources, has_relevant_context, similar_docs = (
                prepare_question_context(request)
            )
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ AI ëª¨ë¸ì„ í˜¸ì¶œí•˜ì§€ ì•Šê³  ì§ì ‘ ë‹µë³€ ë°˜í™˜
        if not has_relevant_context:
            logger.info(f"ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ì–´ AI ëª¨ë¸ì„ í˜¸ì¶œí•˜ì§€ ì•Šê³  ì§ì ‘ ë‹µë³€ ë°˜í™˜ (ì§ˆë¬¸: {request.question[:50]}...)")
            answer = generate_fallback_answer(context_docs, False, has_relevant_context, similar_docs)
            return AskResponse(
                answer=answer,
                context=context_docs,
                sources=sources,
                model_used=None,
                error=None
            )
        
        # Ollama ì‘ë‹µ ìƒì„±
        answer = ""
        model_used = None
        error_message = None
        
        if ollama_available():
            try:
                answer = generate_ai_answer(
                    request.question,
                    context_text,
                    request.max_tokens,
                    request.temperature,
                    has_relevant_context,
                    similar_docs
                )
                model_used = "ollama"
            except Exception as e:
                error_str = str(e)
                # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì´ˆê³¼ ì˜¤ë¥˜ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                if "context window" in error_str.lower() or "exceeds" in error_str.lower():
                    error_message = f"í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ì¤„ì´ê±°ë‚˜ ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”. (ì˜¤ë¥˜: {error_str})"
                    logger.warning(f"ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì´ˆê³¼: {error_str}")
                    # ì»¨í…ìŠ¤íŠ¸ë¥¼ ë” ì¤„ì—¬ì„œ ì¬ì‹œë„
                    if context_docs and has_relevant_context:
                        reduced_context = context_text[:600] if len(context_text) > 600 else context_text
                        try:
                            reduced_prompt = build_prompt(request.question, reduced_context, has_relevant_context, similar_docs)
                            logger.info("ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¤„ì—¬ì„œ ì¬ì‹œë„ ì¤‘...")
                            answer = ollama_generate(
                                reduced_prompt,
                                max_tokens=request.max_tokens,
                                temperature=request.temperature,
                                top_k=40,
                                top_p=0.9,
                                repeat_penalty=1.2,
                            )
                            if answer:
                                answer = postprocess_answer(answer)
                                model_used = "ollama"
                                error_message = None
                                logger.info("ì»¨í…ìŠ¤íŠ¸ ì¶•ì†Œ í›„ ì¬ì‹œë„ ì„±ê³µ")
                            else:
                                raise ValueError("Ollama ì‘ë‹µ ì—†ìŒ")
                        except Exception as retry_e:
                            logger.error(f"ì¬ì‹œë„ë„ ì‹¤íŒ¨: {retry_e}")
                            error_message = f"í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ì¤„ì´ê±°ë‚˜ ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”."
                            answer = generate_fallback_answer(context_docs, True, has_relevant_context, similar_docs)
                else:
                    error_message = f"Ollama ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {error_str}"
                    logger.error(error_message, exc_info=True)
                    answer = generate_fallback_answer(context_docs, True, has_relevant_context, similar_docs)
        else:
            # Ollamaë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš° â€” ê³µí†µ ì—°ê²° í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í”¼ë“œë°±ì— í¬í•¨
            ollama_feedback = ollama_connection_check()
            logger.warning("Ollamaë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì‘ë‹µ ìƒì„±: %s", ollama_feedback.get("message"))
            answer = generate_fallback_answer(context_docs, False, has_relevant_context, similar_docs)
            error_message = ollama_feedback.get("message") or "Ollamaë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            return AskResponse(
                answer=answer,
                context=context_docs,
                sources=sources,
                model_used=model_used,
                error=error_message,
                ollama_feedback=ollama_feedback,
            )
        
        ollama_feedback = ollama_connection_check()
        return AskResponse(
            answer=answer,
            context=context_docs,
            sources=sources,
            model_used=model_used,
            error=error_message,
            ollama_feedback=ollama_feedback if not ollama_feedback.get("available") else None,
        )
        
    except Exception as e:
        logger.error(f"AI ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"AI ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")


async def generate_streaming_answer(
    question: str,
    context_text: str,
    max_tokens: int,
    temperature: float,
    has_relevant_context: bool = True,
    similar_docs: list = None
) -> AsyncGenerator[str, None]:
    """ìŠ¤íŠ¸ë¦¬ë° AI ë‹µë³€ ìƒì„± (Ollama ì „ì²´ ì‘ë‹µ í›„ ì²­í¬ë¡œ ìŠ¤íŠ¸ë¦¬ë°)"""
    if similar_docs is None:
        similar_docs = []
    prompt = build_prompt(question, context_text, has_relevant_context, similar_docs)
    
    try:
        answer = ollama_generate(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_k=40,
            top_p=0.9,
            repeat_penalty=1.2,
        )
        if answer is None:
            answer = ""
        answer = postprocess_answer(answer)
        # ë‹µë³€ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
        chunk_size = 10
        for i in range(0, len(answer), chunk_size):
            chunk = answer[i:i + chunk_size]
            data = {"type": "chunk", "content": chunk}
            yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
        data = {"type": "done", "content": ""}
        yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
    except Exception as e:
        error_data = {"type": "error", "content": str(e)}
        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"


@router.post(
    "/stream",
    summary="AI ì§ˆì˜ ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë°)",
    description="SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ AI ë‹µë³€ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ë°˜í™˜. search_mode, use_reranking, use_multihop ì§€ì›.",
    responses={
        200: {"description": "ì„±ê³µ (SSE ìŠ¤íŠ¸ë¦¼)"},
        400: {"description": "ì§ˆë¬¸ ëˆ„ë½"},
        500: {"description": "AI ì‘ë‹µ ìƒì„± ì˜¤ë¥˜"},
    },
)
async def ask_question_stream(
    request: AskRequest,
    db: Session = Depends(get_db),
):
    """AI ì§ˆì˜ ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë°, Phase 9-3-3: search_mode/use_reranking/use_multihop ì§€ì›)."""
    if not request.question or not request.question.strip():
        raise HTTPException(status_code=400, detail="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

    use_enhanced = (
        request.search_mode == "hybrid"
        or request.use_reranking
        or request.use_multihop
    )

    async def generate():
        try:
            if use_enhanced:
                context_docs, context_text, sources, has_relevant_context, similar_docs = (
                    prepare_question_context_enhanced(request, db)
                )
            else:
                context_docs, context_text, sources, has_relevant_context, similar_docs = (
                    prepare_question_context(request)
                )
            
            # ì†ŒìŠ¤ ì •ë³´ ì „ì†¡
            sources_data = {
                "type": "sources",
                "content": sources
            }
            yield f"data: {json.dumps(sources_data, ensure_ascii=False)}\n\n"
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ AI ëª¨ë¸ì„ í˜¸ì¶œí•˜ì§€ ì•Šê³  ì§ì ‘ ë‹µë³€ ë°˜í™˜
            if not has_relevant_context:
                answer = generate_fallback_answer(context_docs, False, has_relevant_context, similar_docs)
                for i in range(0, len(answer), 10):
                    chunk = answer[i:i + 10]
                    data = {
                        "type": "chunk",
                        "content": chunk
                    }
                    yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                
                data = {
                    "type": "done",
                    "content": ""
                }
                yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                return
            
            if ollama_available():
                async for chunk in generate_streaming_answer(
                    request.question,
                    context_text,
                    request.max_tokens,
                    request.temperature,
                    has_relevant_context,
                    similar_docs
                ):
                    yield chunk
            else:
                # Ollama êº¼ì ¸ ìˆìŒ â€” ê³µí†µ ì—°ê²° í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í”¼ë“œë°± ì´ë²¤íŠ¸ë¡œ ë¨¼ì € ì „ì†¡
                ollama_feedback = ollama_connection_check()
                feedback_data = {"type": "ollama_feedback", "content": ollama_feedback}
                yield f"data: {json.dumps(feedback_data, ensure_ascii=False)}\n\n"
                # í´ë°± ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°
                answer = generate_fallback_answer(
                    context_docs, False, has_relevant_context, similar_docs
                )
                for i in range(0, len(answer), 10):
                    chunk = answer[i:i + 10]
                    data = {
                        "type": "chunk",
                        "content": chunk
                    }
                    yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                
                data = {
                    "type": "done",
                    "content": ""
                }
                yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                
        except Exception as e:
            error_data = {
                "type": "error",
                "content": str(e)
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )
