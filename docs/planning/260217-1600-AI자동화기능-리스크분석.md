# AI 자동화 기능 리스크 분석 (80+ 파일 처리 시)

**작성일:** 2026-02-17 16:00
**작성자:** Backend & Logic Expert (Claude Code)
**대상:** AI 자동화 6단계 파이프라인 (`/api/automation/run-full-workflow`)

---

## 1. 현황 분석

### 1.1 현재 파이프라인 6단계

| 단계 | 기능 | 진행률 | 주요 처리 |
|------|------|--------|-----------|
| 1 | 텍스트 추출 | 0-15% | 파일 I/O + 파서 (PDF/DOCX/HWP 등) |
| 2 | 청크 생성 | 15-30% | 1000자 단위 분할 + DB INSERT |
| 3 | 키워드 추출 | 30-50% | Ollama LLM 호출 (청크당 1회) |
| 4 | 라벨 매칭 | 50-70% | 전체 라벨 로드 + 텍스트 매칭 |
| 5 | 승인 처리 | 70-80% | DB UPDATE (auto_approve 시) |
| 6 | Qdrant 임베딩 | 80-100% | SentenceTransformer + Qdrant upsert |

### 1.2 80 파일 처리 시 예상 수치

| 항목 | 산출 근거 | 예상 값 |
|------|-----------|---------|
| 평균 파일 크기 | Markdown/PDF 기준 | 5~20KB |
| 파일당 청크 수 | 1000자 기준 | 5~20개 |
| **총 청크 수** | 80 x 평균 12개 | **~960개** |
| 키워드 추출 LLM 호출 | 청크당 1회 | **~960회** |
| Qdrant 임베딩 | 청크당 1회 | **~960회** |
| 라벨 매칭 비교 | 청크 x 전체라벨 | 960 x N (라벨 수) |

---

## 2. 리스크 식별

### 2.1 시스템 리소스 리스크

| 리스크 | 심각도 | 현재 상태 | 80+ 파일 시 |
|--------|--------|-----------|-------------|
| **메모리 누적** | HIGH | 모든 청크를 메모리에 누적 후 batch commit | 960개 청크 객체 + 텍스트 = 50~200MB |
| **Ollama LLM 병목** | CRITICAL | 청크당 순차 호출, rate limit 10/min | 960회 / 10/min = **최소 96분** |
| **DB 커넥션 점유** | MEDIUM | 단일 세션으로 전체 처리 | 긴 트랜잭션으로 커넥션 풀 고갈 위험 |
| **Qdrant 단건 처리** | HIGH | `sync_chunk_to_qdrant()` 청크당 1회 | 960회 HTTP 요청 = 네트워크 오버헤드 |
| **인메모리 태스크 상태** | MEDIUM | 서버 재시작 시 유실 | 장시간 처리 중 서버 재시작 → 전체 유실 |

### 2.2 프론트엔드 렌더링 리스크

| 리스크 | 심각도 | 설명 |
|--------|--------|------|
| **SSE 연결 타임아웃** | HIGH | 80+ 파일 처리 시 수십 분~수 시간 소요, 브라우저/프록시 SSE 타임아웃 |
| **문서 리스트 DOM 과부하** | MEDIUM | 500개 문서 체크박스 한 번에 렌더링 |
| **진행률 UI 정체** | MEDIUM | LLM 호출 대기 중 0.5초 폴링에도 업데이트 없음 → UX 저하 |
| **결과 패널 대량 데이터** | LOW | 최종 결과 수치만 표시 (상세 미표시) |

### 2.3 처리 안정성 리스크

| 리스크 | 심각도 | 설명 |
|--------|--------|------|
| **중간 실패 시 전체 롤백** | CRITICAL | 단계 5에서 실패하면 단계 1~4 결과도 활용 불가 |
| **취소 후 잔여 데이터** | MEDIUM | 청크 생성 후 취소 시 draft 청크 잔류 |
| **동시 실행 충돌** | HIGH | 동일 문서에 대해 다중 워크플로우 실행 시 중복 청크 |

---

## 3. 개선 방안

### 3.1 UI 측면 해결 방안

#### 방안 A: 가상 스크롤(Virtual Scroll) 문서 리스트

**문제:** 500개 문서 체크박스를 한 번에 DOM에 렌더링 → DOM 노드 과다

**해결:**
```javascript
// 현재: 전체 렌더링
allDocuments.forEach(doc => container.appendChild(createDocItem(doc)));

// 개선: 가시 영역만 렌더링 (Virtual Scroll)
const VISIBLE_COUNT = 20;
const ITEM_HEIGHT = 36; // px

function renderVisibleDocs(scrollTop) {
  const startIdx = Math.floor(scrollTop / ITEM_HEIGHT);
  const endIdx = Math.min(startIdx + VISIBLE_COUNT + 2, allDocuments.length);

  container.style.height = (allDocuments.length * ITEM_HEIGHT) + 'px';
  container.innerHTML = '';

  for (let i = startIdx; i < endIdx; i++) {
    const el = createDocItem(allDocuments[i]);
    el.style.position = 'absolute';
    el.style.top = (i * ITEM_HEIGHT) + 'px';
    container.appendChild(el);
  }
}
```

**효과:** DOM 노드 500개 → 22개로 감소 (96% 절감)

#### 방안 B: 단계별 세부 진행률 표시

**문제:** LLM 호출 중 진행률이 멈춰 보이는 UX 저하

**해결:**
```javascript
// 현재: 단계 레벨 진행률만 표시
updateProgress(data.progress_pct);

// 개선: 단계 내 세부 진행률 + 청크 카운터
eventSource.addEventListener('progress', (event) => {
  const d = JSON.parse(event.data);
  updateProgress(d.progress_pct);

  // 세부 정보 표시
  if (d.detail) {
    updateDetail(`${d.detail.current}/${d.detail.total} 처리 중 (${d.detail.item_name})`);
    updateSubProgress(d.detail.current / d.detail.total * 100);
  }

  // 예상 소요 시간 표시
  if (d.eta_seconds) {
    updateETA(formatDuration(d.eta_seconds));
  }
});
```

**효과:** 사용자가 처리 진행 상황을 실시간으로 확인 가능, 대기 불안감 해소

#### 방안 C: 파일 선택 시 사전 검증 UI

**문제:** 80개 이상 선택 후 실행 시 장시간 소요 사전 인지 불가

**해결:**
```javascript
function updateSelectionSummary() {
  const selected = getSelectedDocIds();
  const estimatedChunks = selected.length * 12; // 평균 12청크
  const estimatedMinutes = Math.ceil(estimatedChunks / 10); // LLM rate limit 기준

  summaryEl.innerHTML = `
    <div class="selection-warning ${selected.length > 50 ? 'warn' : ''}">
      선택: ${selected.length}개 문서 | 예상 청크: ~${estimatedChunks}개
      | 예상 소요: ~${estimatedMinutes}분
      ${selected.length > 50 ? '⚠️ 대량 처리 - 배치 분할 권장' : ''}
    </div>`;
}
```

**효과:** 사용자가 실행 전 리소스 영향을 사전 파악

#### 방안 D: SSE 재연결 + Heartbeat

**문제:** 장시간 SSE 연결이 프록시/브라우저에 의해 끊김

**해결:**
```javascript
// 현재: 단순 EventSource
const es = new EventSource(url);

// 개선: 재연결 + Heartbeat 감지
let lastEventTime = Date.now();
const HEARTBEAT_TIMEOUT = 30000; // 30초

const es = new EventSource(url);
es.addEventListener('heartbeat', () => { lastEventTime = Date.now(); });
es.onerror = () => {
  es.close();
  setTimeout(() => reconnectSSE(taskId), 3000); // 3초 후 재연결
};

// Heartbeat 감시
setInterval(() => {
  if (Date.now() - lastEventTime > HEARTBEAT_TIMEOUT) {
    es.close();
    reconnectSSE(taskId);
  }
}, 10000);
```

백엔드 heartbeat 추가:
```python
# stream_progress() 내
async def stream_progress(task_id):
    while True:
        await asyncio.sleep(0.5)
        yield format_sse({"type": "heartbeat"}, event="heartbeat")
        # ... 기존 로직
```

**효과:** 장시간 처리 중 연결 유실 방지

---

### 3.2 백엔드 알고리즘 개선 방안

#### 방안 E: Qdrant 배치 임베딩 (Batch Upsert)

**문제:** 청크당 개별 Qdrant 호출 → 960회 HTTP 요청

**해결:**
```python
# 현재: 개별 처리
for chunk in approved_chunks:
    sync_chunk_to_qdrant(db, chunk.id)  # 1건씩

# 개선: 배치 처리 (50건 단위)
BATCH_SIZE = 50

async def _embed_chunks_batch(self, db, task_id, chunk_ids):
    chunks = db.query(KnowledgeChunk).filter(
        KnowledgeChunk.id.in_(chunk_ids),
        KnowledgeChunk.status == "approved"
    ).all()

    for i in range(0, len(chunks), BATCH_SIZE):
        batch = chunks[i:i + BATCH_SIZE]

        # 배치 임베딩 생성
        texts = [c.content for c in batch]
        embeddings = embedding_model.encode(texts, batch_size=BATCH_SIZE)

        # 배치 Qdrant upsert
        points = [
            PointStruct(id=c.id, vector=emb.tolist(), payload={...})
            for c, emb in zip(batch, embeddings)
        ]
        qdrant_client.upsert(collection_name=COLLECTION, points=points)

        # 배치 DB 업데이트
        for c in batch:
            c.qdrant_point_id = str(c.id)
        db.commit()

        self._update_progress(task_id, ...)
```

**효과:**
- HTTP 요청: 960회 → 20회 (98% 감소)
- SentenceTransformer 배치 연산으로 GPU/CPU 활용 극대화
- 예상 속도 개선: 10~20배

#### 방안 F: LLM 키워드 추출 최적화

**문제:** Ollama rate limit 10/min → 960 청크 = 96분

**해결 옵션:**

| 옵션 | 방법 | 예상 효과 |
|------|------|-----------|
| F-1 | 청크 묶음 프롬프트 (5~10개 청크를 1회 호출) | 960회 → 96~192회 |
| F-2 | Regex/TF-IDF 폴백 (LLM 실패/지연 시) | LLM 의존도 제거 |
| F-3 | Rate limit 상향 + Ollama 병렬 인스턴스 | 10/min → 60/min |
| F-4 | 키워드 캐시 (동일 내용 중복 추출 방지) | 유사 문서 20~30% 절감 |

**F-1 구현 예시:**
```python
# 현재: 청크당 1회 LLM 호출
for chunk in chunks:
    keywords = ollama_extract_keywords(chunk.content)

# 개선: 5개 청크 묶음 호출
KEYWORD_BATCH = 5
for i in range(0, len(chunks), KEYWORD_BATCH):
    batch = chunks[i:i + KEYWORD_BATCH]
    combined_prompt = "\n---\n".join(
        f"[문서 {j+1}]\n{c.content[:500]}" for j, c in enumerate(batch)
    )
    result = ollama_generate(
        f"다음 {len(batch)}개 텍스트에서 각각 5개 키워드를 추출하세요:\n{combined_prompt}"
    )
    # 결과 파싱 후 각 청크에 할당
```

**효과:** LLM 호출 960회 → 192회 (80% 감소), 소요시간 96분 → 20분

#### 방안 G: 라벨 매칭 인덱스 최적화

**문제:** 모든 라벨을 메모리에 로드 후 전체 청크와 O(N*M) 비교

**해결:**
```python
# 현재: 브루트포스 매칭
all_labels = db.query(Label).filter(Label.label_type.in_([...])).all()
for chunk in chunks:
    for label in all_labels:
        if label.name.lower() in chunk.content.lower():
            # match

# 개선: 역인덱스 기반 매칭
from collections import defaultdict

label_index = defaultdict(list)  # keyword → [label_id, ...]
for label in all_labels:
    for token in label.name.lower().split():
        label_index[token].append(label)

for chunk in chunks:
    chunk_tokens = set(chunk.content.lower().split())
    candidate_labels = set()
    for token in chunk_tokens:
        if token in label_index:
            candidate_labels.update(label_index[token])

    for label in candidate_labels:
        if label.name.lower() in chunk.content.lower():
            # confirmed match
```

**효과:** O(N*M) → O(N*K) (K = 평균 토큰 히트 수, M >> K)

#### 방안 H: DB 트랜잭션 분할

**문제:** 전체 처리를 단일 세션으로 → 커넥션 장시간 점유

**해결:**
```python
# 현재: 단일 세션
def execute_workflow(self, db, task_id, ...):
    self._extract_texts(db, ...)    # 세션 유지
    self._create_chunks(db, ...)    # 세션 유지
    self._extract_keywords(db, ...) # 세션 유지 (수십 분)
    db.commit()

# 개선: 단계별 세션 분리
def execute_workflow(self, task_id, ...):
    with SessionLocal() as db:
        texts = self._extract_texts(db, ...)

    with SessionLocal() as db:
        chunk_ids = self._create_chunks(db, texts, ...)
        db.commit()

    with SessionLocal() as db:
        self._extract_keywords(db, chunk_ids, ...)
        db.commit()
    # ...
```

**효과:** 커넥션 점유 시간 분산, 단계 간 독립 커밋으로 중간 실패 시 부분 보존

---

### 3.3 파일 순차(배치) 처리 방안

#### 방안 I: 자동 배치 분할 실행

**문제:** 80개 이상 파일을 한 번에 처리 → 리소스 폭주

**해결:**
```python
# backend/services/automation/ai_workflow_service.py

BATCH_SIZE = 20  # 배치당 문서 수

async def execute_workflow_batched(self, task_id, document_ids, options):
    """대량 문서를 배치로 분할하여 순차 처리"""
    total_docs = len(document_ids)
    batches = [
        document_ids[i:i + BATCH_SIZE]
        for i in range(0, total_docs, BATCH_SIZE)
    ]

    all_results = {
        "chunks_created": 0, "keywords_extracted": 0,
        "labels_matched": 0, "chunks_approved": 0, "chunks_embedded": 0
    }

    for batch_idx, batch_ids in enumerate(batches):
        # 배치 진행률 계산
        batch_base_pct = int((batch_idx / len(batches)) * 100)
        batch_range_pct = int(100 / len(batches))

        ai_workflow_state.update_progress(
            task_id,
            stage_name=f"배치 {batch_idx + 1}/{len(batches)}",
            progress_pct=batch_base_pct,
            message=f"문서 {len(batch_ids)}개 처리 중 ({batch_idx * BATCH_SIZE + 1}~{min((batch_idx + 1) * BATCH_SIZE, total_docs)}/{total_docs})"
        )

        # 배치 단위로 6단계 실행
        with SessionLocal() as db:
            batch_result = self._execute_single_batch(
                db, task_id, batch_ids, options,
                progress_offset=batch_base_pct,
                progress_range=batch_range_pct
            )

        # 결과 누적
        for key in all_results:
            all_results[key] += batch_result.get(key, 0)

        # GC 유도 — 배치 간 메모리 해제
        import gc
        gc.collect()

    ai_workflow_state.complete_task(task_id, all_results)
```

**진행률 구조 변경:**
```
배치 1/4: [Stage1 → Stage2 → Stage3 → Stage4 → Stage5 → Stage6] 0~25%
배치 2/4: [Stage1 → Stage2 → Stage3 → Stage4 → Stage5 → Stage6] 25~50%
배치 3/4: [Stage1 → Stage2 → Stage3 → Stage4 → Stage5 → Stage6] 50~75%
배치 4/4: [Stage1 → Stage2 → Stage3 → Stage4 → Stage5 → Stage6] 75~100%
```

**효과:**
- 메모리 사용량: 전체 → 1/4 수준으로 분산
- 배치 간 GC로 메모리 누적 방지
- 중간 배치 실패 시 이전 배치 결과 보존

#### 방안 J: 큐 기반 비동기 처리 (Celery/RQ)

**문제:** 인메모리 태스크 → 서버 재시작 시 유실, 단일 프로세스 병목

**해결 아키텍처:**
```
[Frontend] → POST /run-full-workflow
    ↓
[FastAPI] → task_id 생성 → DB workflow_tasks 저장 → Celery task 발행
    ↓
[Redis Queue] → Worker 1: 배치 1~2 처리
              → Worker 2: 배치 3~4 처리
    ↓
[Worker] → 단계별 DB 업데이트 → SSE/WebSocket 진행률 전송
    ↓
[Frontend] ← SSE 수신 (task 상태 DB 폴링 또는 WebSocket)
```

**DB 태스크 테이블:**
```sql
CREATE TABLE workflow_tasks (
    id SERIAL PRIMARY KEY,
    task_id VARCHAR(36) UNIQUE NOT NULL,
    status VARCHAR(20) DEFAULT 'pending',  -- pending/running/completed/failed/cancelled
    document_ids JSONB NOT NULL,
    options JSONB,
    progress_pct INT DEFAULT 0,
    current_stage VARCHAR(50),
    results JSONB,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

**효과:**
- 서버 재시작 무관 (태스크 DB 영속화)
- 멀티 워커 수평 확장 가능
- 복잡도 증가 (Redis + Celery 인프라 필요)

#### 방안 K: 파일별 완료 즉시 반영 (Streaming Results)

**문제:** 전체 처리 완료까지 결과 확인 불가

**해결:**
```python
# 파일 단위로 완료 이벤트 발행
def _process_single_document(self, db, task_id, doc_id, doc_text):
    """단일 문서 6단계 처리 + 즉시 결과 발행"""
    # 1. 청크 생성
    chunks = self._create_chunks_for_doc(db, doc_id, doc_text)
    db.commit()

    # 2. 키워드 추출
    self._extract_keywords_for_chunks(db, chunks)
    db.commit()

    # 3. 라벨 매칭
    self._match_labels_for_chunks(db, chunks)
    db.commit()

    # 4. 승인 + 임베딩
    if self.auto_approve:
        self._approve_and_embed(db, chunks)
        db.commit()

    # 즉시 결과 발행
    ai_workflow_state.emit_document_result(task_id, {
        "document_id": doc_id,
        "status": "completed",
        "chunks_created": len(chunks),
        "keywords": sum(c.keyword_count for c in chunks),
    })
```

프론트엔드 수신:
```javascript
eventSource.addEventListener('doc_result', (event) => {
  const d = JSON.parse(event.data);
  // 완료된 문서를 결과 패널에 즉시 추가
  appendDocResult(d.document_id, d.chunks_created, d.keywords);
  // 문서 리스트에서 완료 표시
  markDocCompleted(d.document_id);
});
```

**효과:** 사용자가 처리 중에도 완료된 문서 결과를 즉시 확인 가능

---

## 4. 우선순위 로드맵

### Phase 1 (즉시 적용 가능 — 코드 변경 최소)

| 순위 | 방안 | 효과 | 난이도 |
|------|------|------|--------|
| 1 | **C: 사전 검증 UI** | 사용자 인지 개선 | LOW |
| 2 | **B: 세부 진행률** | UX 개선 | LOW |
| 3 | **H: DB 트랜잭션 분할** | 커넥션 안정성 | LOW |
| 4 | **D: SSE Heartbeat** | 연결 안정성 | LOW |

### Phase 2 (중기 — 성능 핵심 개선)

| 순위 | 방안 | 효과 | 난이도 |
|------|------|------|--------|
| 5 | **E: Qdrant 배치 임베딩** | 임베딩 10~20배 가속 | MEDIUM |
| 6 | **F-1: LLM 묶음 호출** | 키워드 추출 5배 가속 | MEDIUM |
| 7 | **I: 자동 배치 분할** | 메모리 안정화 | MEDIUM |
| 8 | **G: 라벨 역인덱스** | 매칭 속도 개선 | LOW |

### Phase 3 (장기 — 인프라 확장)

| 순위 | 방안 | 효과 | 난이도 |
|------|------|------|--------|
| 9 | **K: Streaming Results** | UX 대폭 개선 | MEDIUM |
| 10 | **A: Virtual Scroll** | DOM 성능 최적화 | MEDIUM |
| 11 | **J: Celery 큐 도입** | 수평 확장 가능 | HIGH |

---

## 5. 예상 효과 종합

| 지표 | 현재 (80 파일) | Phase 1 후 | Phase 2 후 |
|------|----------------|-----------|-----------|
| **총 소요 시간** | ~100분+ | ~100분 (변화 없음) | **~15분** |
| **메모리 피크** | ~200MB | ~200MB | **~50MB** |
| **DB 커넥션 점유** | 전체 시간 | 단계별 분리 | 배치별 분리 |
| **SSE 안정성** | 끊김 위험 | Heartbeat 보장 | 보장 |
| **UX 인지** | 진행 불투명 | 세부 진행률 | 문서별 즉시 결과 |
| **중간 실패 복구** | 전체 유실 | 단계별 보존 | **배치별 보존** |

---

## 6. 결론

80개 이상 파일 처리 시 가장 큰 병목은 **Ollama LLM 키워드 추출**(rate limit 10/min)과 **Qdrant 단건 임베딩**입니다.

**즉시 적용 권장:**
1. 사전 검증 UI + 세부 진행률 (사용자 경험 개선)
2. DB 트랜잭션 분할 (안정성)

**핵심 성능 개선:**
3. Qdrant 배치 임베딩 (10~20배 가속)
4. LLM 묶음 호출 (5배 가속)
5. 자동 배치 분할 20건 단위 (메모리 안정화)

이 3가지 백엔드 개선만으로 총 소요시간을 **100분+ → 15분** 수준으로 단축할 수 있습니다.
