#!/usr/bin/env python3
"""
GPT4All ëª¨ë¸ ì„¤ì¹˜ ì •ë³´ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—°ê²° í™•ì¸ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python scripts/check_gpt4all_model.py
"""

import sys
import os
from pathlib import Path

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def check_gpt4all_installation():
    """GPT4All íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸"""
    print("="*60)
    print("1. GPT4All íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸")
    print("="*60)
    
    try:
        import gpt4all
        print(f"âœ… gpt4all íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¨")
        
        # ë²„ì „ í™•ì¸
        try:
            version = gpt4all.__version__
            print(f"   ë²„ì „: {version}")
        except AttributeError:
            print("   ë²„ì „: í™•ì¸ ë¶ˆê°€")
        
        return True
    except ImportError:
        print("âŒ gpt4all íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì„¤ì¹˜ ë°©ë²•: pip install gpt4all")
        return False


def check_model_info():
    """ëª¨ë¸ ì •ë³´ í™•ì¸"""
    print("\n" + "="*60)
    print("2. ëª¨ë¸ ì •ë³´ í™•ì¸")
    print("="*60)
    
    model_name = "Meta-Llama-3-8B-Instruct.Q4_0.gguf"
    print(f"ëª¨ë¸ ì´ë¦„: {model_name}")
    print(f"ëª¨ë¸ í¬ê¸°: ì•½ 4.66 GB")
    print(f"í•„ìš” RAM: ì•½ 8 GB")
    print(f"íŒŒë¼ë¯¸í„°: 8B")
    
    return model_name


def check_model_storage_location():
    """ëª¨ë¸ ì €ì¥ ìœ„ì¹˜ í™•ì¸"""
    print("\n" + "="*60)
    print("3. ëª¨ë¸ ì €ì¥ ìœ„ì¹˜ í™•ì¸")
    print("="*60)
    
    # GPT4All ê¸°ë³¸ ì €ì¥ ìœ„ì¹˜
    home_dir = Path.home()
    cache_dir = home_dir / ".cache" / "gpt4all"
    
    print(f"ê¸°ë³¸ ì €ì¥ ìœ„ì¹˜: {cache_dir}")
    
    if cache_dir.exists():
        print(f"âœ… ìºì‹œ ë””ë ‰í† ë¦¬ ì¡´ì¬")
        
        # ëª¨ë¸ íŒŒì¼ í™•ì¸
        model_files = list(cache_dir.glob("*.gguf"))
        model_files.extend(list(cache_dir.glob("*.bin")))
        
        if model_files:
            print(f"\nğŸ“ ë°œê²¬ëœ ëª¨ë¸ íŒŒì¼ ({len(model_files)}ê°œ):")
            for model_file in model_files:
                size_mb = model_file.stat().st_size / (1024 * 1024)
                size_gb = size_mb / 1024
                print(f"   - {model_file.name} ({size_gb:.2f} GB)")
        else:
            print("âš ï¸  ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì²« ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.")
    else:
        print("âš ï¸  ìºì‹œ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì²« ì‹¤í–‰ ì‹œ ìƒì„±ë©ë‹ˆë‹¤.")
    
    return cache_dir


def check_code_connections():
    """ì½”ë“œì—ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—°ê²° í™•ì¸"""
    print("\n" + "="*60)
    print("4. ì½”ë“œì—ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—°ê²° í™•ì¸")
    print("="*60)
    
    files_to_check = [
        ("backend/routers/ai.py", "get_gpt4all_model()"),
        ("backend/services/system_service.py", "_get_gpt4all_status()"),
        ("scripts/embed_and_store.py", "extract_title_with_ai()"),
        ("scripts/extract_keywords_and_labels.py", "extract_keywords_with_gpt4all()"),
        ("scripts/search_and_query.py", "query_with_gpt4all()"),
        ("scripts/generate_chunk_titles.py", "extract_title_with_ai() (import)"),
    ]
    
    model_name = "Meta-Llama-3-8B-Instruct.Q4_0.gguf"
    
    print(f"ì‚¬ìš© ëª¨ë¸: {model_name}\n")
    
    for file_path, function_name in files_to_check:
        full_path = PROJECT_ROOT / file_path
        if full_path.exists():
            # íŒŒì¼ì—ì„œ ëª¨ë¸ ì´ë¦„ í™•ì¸
            content = full_path.read_text(encoding='utf-8')
            if model_name in content:
                print(f"âœ… {file_path}")
                print(f"   í•¨ìˆ˜: {function_name}")
            elif "GPT4All" in content or "gpt4all" in content:
                print(f"âš ï¸  {file_path}")
                print(f"   í•¨ìˆ˜: {function_name}")
                print(f"   ëª¨ë¸ ì´ë¦„ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš© ì¤‘")
            else:
                print(f"â„¹ï¸  {file_path}")
                print(f"   GPT4All ì‚¬ìš© ì•ˆ í•¨")
        else:
            print(f"âŒ {file_path} - íŒŒì¼ ì—†ìŒ")


def test_model_loading():
    """ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("5. ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    try:
        from gpt4all import GPT4All
        
        model_name = "Meta-Llama-3-8B-Instruct.Q4_0.gguf"
        print(f"ëª¨ë¸ ë¡œë”© ì‹œë„: {model_name}")
        print("â³ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ë¡œë”© ì¤‘... (ì²˜ìŒ ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        
        try:
            model = GPT4All(model_name)
            print("âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            print("\nê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
            response = model.generate("Hello", max_tokens=10, temp=0.1)
            print(f"âœ… ìƒì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ: '{response[:50]}...'")
            
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print("\nê°€ëŠ¥í•œ ì›ì¸:")
            print("  - ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì¸í„°ë„· ì—°ê²° í™•ì¸)")
            print("  - ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±")
            print("  - ë©”ëª¨ë¦¬ ë¶€ì¡±")
            return False
            
    except ImportError:
        print("âŒ gpt4all íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("GPT4All ëª¨ë¸ ì„¤ì¹˜ ì •ë³´ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—°ê²° í™•ì¸")
    print("="*60 + "\n")
    
    # 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
    is_installed = check_gpt4all_installation()
    
    if not is_installed:
        print("\nâš ï¸  gpt4all íŒ¨í‚¤ì§€ë¥¼ ë¨¼ì € ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("   pip install gpt4all")
        return
    
    # 2. ëª¨ë¸ ì •ë³´
    model_name = check_model_info()
    
    # 3. ì €ì¥ ìœ„ì¹˜ í™•ì¸
    cache_dir = check_model_storage_location()
    
    # 4. ì½”ë“œ ì—°ê²° í™•ì¸
    check_code_connections()
    
    # 5. ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ (ì„ íƒì )
    print("\n" + "="*60)
    user_input = input("ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
    if user_input == 'y':
        test_model_loading()
    else:
        print("ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    print("\n" + "="*60)
    print("í™•ì¸ ì™„ë£Œ!")
    print("="*60)
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì²« ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤")
    print("2. ì œëª© ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰: python scripts/generate_chunk_titles.py --limit 5")
    print("3. ëŒ€ì‹œë³´ë“œì—ì„œ GPT4All ìƒíƒœ í™•ì¸: http://localhost:8000/dashboard")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
