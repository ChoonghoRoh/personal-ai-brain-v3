#!/usr/bin/env python3
"""
docs í´ë”ì˜ .md íŒŒì¼ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ë¼ë²¨ì„ ìë™ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import sys
from pathlib import Path
from typing import List, Dict, Set, Optional
import re
from collections import Counter
from sqlalchemy.orm import Session

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from backend.models.database import SessionLocal
from backend.models.models import Label, KnowledgeChunk, KnowledgeLabel, Document

DOCS_DIR = PROJECT_ROOT / "docs"
BRAIN_DIR = PROJECT_ROOT / "brain"


def extract_keywords_from_markdown(content: str, top_n: int = 10, use_llm: bool = False) -> List[str]:
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ

    Args:
        content: ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë‚´ìš©
        top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
        use_llm: LLMì„ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ì—¬ë¶€ (Trueë©´ LLM ì‚¬ìš©, Falseë©´ ì •ê·œì‹ ê¸°ë°˜)
    """
    if use_llm:
        return extract_keywords_with_llm(content, top_n)
    else:
        return extract_keywords_with_regex(content, top_n)


def extract_keywords_with_regex(content: str, top_n: int = 10) -> List[str]:
    """ì •ê·œì‹ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ë³¸ ë°©ë²•)"""
    # í•œê¸€ ëª…ì‚¬ íŒ¨í„´ (ê°„ë‹¨í•œ ë²„ì „)
    korean_noun_pattern = r'[ê°€-í£]{2,}'

    # ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì œê±°
    content = re.sub(r'#+\s*', '', content)  # í—¤ë” ì œê±°
    content = re.sub(r'\*\*([^*]+)\*\*', r'\1', content)  # ë³¼ë“œ ì œê±°
    content = re.sub(r'\*([^*]+)\*', r'\1', content)  # ì´íƒ¤ë¦­ ì œê±°
    content = re.sub(r'`([^`]+)`', r'\1', content)  # ì½”ë“œ ì œê±°
    content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)  # ë§í¬ ì œê±°

    # í•œê¸€ ëª…ì‚¬ ì¶”ì¶œ
    nouns = re.findall(korean_noun_pattern, content)

    # ë¹ˆë„ìˆ˜ ê³„ì‚°
    noun_counter = Counter(nouns)

    # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨í•œ ë²„ì „)
    stopwords = {'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë•Œ', 'ì´', 'ê·¸', 'ì €', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì™€', 'ê³¼'}
    filtered_nouns = [(noun, count) for noun, count in noun_counter.items()
                      if noun not in stopwords and len(noun) >= 2]

    # ë¹ˆë„ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    filtered_nouns.sort(key=lambda x: x[1], reverse=True)

    # ìƒìœ„ Nê°œ ë°˜í™˜
    return [noun for noun, _ in filtered_nouns[:top_n]]


def extract_keywords_with_llm(content: str, top_n: int = 10) -> List[str]:
    """LLMì„ í™œìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¬¸ë§¥ ì´í•´ ê¸°ë°˜)"""
    # ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì œê±° (LLMì— ì „ë‹¬í•˜ê¸° ì „ ì •ë¦¬)
    cleaned_content = re.sub(r'#+\s*', '', content)
    cleaned_content = re.sub(r'\*\*([^*]+)\*\*', r'\1', cleaned_content)
    cleaned_content = re.sub(r'\*([^*]+)\*', r'\1', cleaned_content)

    # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì‚¬ìš© (í† í° ì œí•œ ê³ ë ¤)
    # ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ê°€ 2048 í† í°ì´ë¯€ë¡œ ì—¬ìœ ë¥¼ ë‘ê³  1000ìë¡œ ì œí•œ
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì´ ì•½ 200ì, ë‹µë³€ ìƒì„±ì„ ìœ„í•´ 200ì ì—¬ìœ 
    max_length = 1000
    if len(cleaned_content) > max_length:
        cleaned_content = cleaned_content[:max_length] + "..."

    # LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = f"""ë‹¤ìŒ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{cleaned_content}

ìš”êµ¬ì‚¬í•­:
1. ë¬¸ì„œì˜ ì£¼ìš” ì£¼ì œì™€ ê´€ë ¨ëœ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
2. ë¶ˆìš©ì–´(ê²ƒ, ìˆ˜, ë“±, ë•Œ ë“±)ëŠ” ì œì™¸
3. ì¼ë°˜ì ì¸ ë‹¨ì–´ë³´ë‹¤ ë¬¸ì„œì— íŠ¹í™”ëœ ì „ë¬¸ ìš©ì–´ë‚˜ ê°œë…ì„ ìš°ì„ 
4. í‚¤ì›Œë“œëŠ” í•œê¸€ë¡œ, 2ê¸€ì ì´ìƒ
5. ìƒìœ„ {top_n}ê°œë§Œ ì¶”ì¶œ
6. ì¤‘êµ­ì–´(ä¸­æ–‡)ë‚˜ ì¼ë³¸ì–´ë¡œ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”

í‚¤ì›Œë“œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•´ì£¼ì„¸ìš”. ì„¤ëª… ì—†ì´ í‚¤ì›Œë“œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
ì˜ˆì‹œ: í”„ë¡œì íŠ¸, ê°œë°œ, ì‹œìŠ¤í…œ, API, ë°ì´í„°ë² ì´ìŠ¤"""

    try:
        # ë°©ë²• 1: Ollama ì‚¬ìš© (ë¡œì»¬ LLM ìš°ì„ )
        return extract_keywords_with_ollama(prompt, top_n)
    except Exception as e:
        print(f"âš ï¸  Ollama ì˜¤ë¥˜: {e}, OpenAI APIë¡œ ëŒ€ì²´ ì‹œë„...")
        try:
            # ë°©ë²• 2: OpenAI API ì‚¬ìš© (í´ë°±)
            return extract_keywords_with_openai(prompt, top_n)
        except Exception as e2:
            print(f"âš ï¸  OpenAI API ì˜¤ë¥˜: {e2}, ì •ê·œì‹ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´...")
            # ë°©ë²• 3: ì •ê·œì‹ ê¸°ë°˜ìœ¼ë¡œ í´ë°±
            return extract_keywords_with_regex(content, top_n)


def extract_keywords_with_openai(prompt: str, top_n: int = 10) -> List[str]:
    """OpenAI APIë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    import os
    try:
        from openai import OpenAI
    except ImportError:
        raise ImportError("openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openai")

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    client = OpenAI(api_key=api_key)

    response = client.chat.completions.create(
        model="gpt-4o-mini",  # ë˜ëŠ” "gpt-3.5-turbo" (ë¹„ìš© ì ˆê°)
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¬¸ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,  # ì¼ê´€ì„± ìˆëŠ” ê²°ê³¼ë¥¼ ìœ„í•´ ë‚®ì€ temperature
        max_tokens=200
    )

    # ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords_text = response.choices[0].message.content.strip()
    keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]

    return keywords[:top_n]


def extract_keywords_with_ollama(prompt: str, top_n: int = 10, model: Optional[str] = None) -> List[str]:
    """Ollamaë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¡œì»¬ LLM, EEVE-Korean ë“±)"""
    try:
        from backend.services.ai.ollama_client import ollama_generate
    except ImportError:
        raise ImportError("backend.services.ai.ollama_clientë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PYTHONPATHì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")

    from backend.config import OLLAMA_MODEL_LIGHT
    use_model = model or OLLAMA_MODEL_LIGHT

    response = ollama_generate(
        prompt,
        max_tokens=200,
        temperature=0.3,
        top_p=0.9,
        timeout=60.0,
        model=use_model,
    )
    if not response:
        raise RuntimeError("Ollama ì‘ë‹µ ì—†ìŒ (ì„œë¹„ìŠ¤ ì‹¤í–‰ ì—¬ë¶€ ë° ëª¨ë¸ ë¡œë“œ í™•ì¸)")

    from backend.utils.korean_utils import postprocess_korean_keywords
    keywords = postprocess_korean_keywords(response)
    return keywords[:top_n]


def extract_keywords_with_gpt4all(prompt: str, top_n: int = 10, model: Optional[str] = None) -> List[str]:
    """GPT4All í˜¸í™˜ ì´ë¦„ â€” ì‹¤ì œë¡œëŠ” Ollama í˜¸ì¶œ (í•˜ìœ„ í˜¸í™˜ìš©)."""
    return extract_keywords_with_ollama(prompt, top_n, model=model)


def process_docs_files(use_llm: bool = False, include_brain: bool = True) -> Dict[str, List[str]]:
    """docs í´ë” ë° brain í´ë”ì˜ ëª¨ë“  .md íŒŒì¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ

    Args:
        use_llm: LLMì„ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ì—¬ë¶€ (ê¸°ë³¸ê°’: False, ì •ê·œì‹ ê¸°ë°˜)
        include_brain: brain í´ë”ë„ ì²˜ë¦¬í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    """
    file_keywords = {}

    # docs í´ë” ì²˜ë¦¬
    for md_file in DOCS_DIR.rglob("*.md"):
        if md_file.is_file():
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                keywords = extract_keywords_from_markdown(content, use_llm=use_llm)
                relative_path = str(md_file.relative_to(PROJECT_ROOT))
                file_keywords[relative_path] = keywords

                method = "LLM" if use_llm else "ì •ê·œì‹"
                print(f"âœ… [{method}] {relative_path}: {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ")
                if keywords:
                    print(f"   í‚¤ì›Œë“œ: {', '.join(keywords[:5])}{'...' if len(keywords) > 5 else ''}")
            except Exception as e:
                print(f"âŒ {md_file}: {e}")

    # brain í´ë”ë„ ì²˜ë¦¬ (ê¸°ì¡´ íŒŒì¼ í¬í•¨)
    if include_brain:
        for md_file in BRAIN_DIR.rglob("*.md"):
            if md_file.is_file():
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    keywords = extract_keywords_from_markdown(content, use_llm=use_llm)
                    relative_path = str(md_file.relative_to(PROJECT_ROOT))
                    
                    # ì´ë¯¸ docsì—ì„œ ì²˜ë¦¬í•œ íŒŒì¼ì´ë©´ ìŠ¤í‚µ
                    if relative_path not in file_keywords:
                        file_keywords[relative_path] = keywords

                        method = "LLM" if use_llm else "ì •ê·œì‹"
                        print(f"âœ… [{method}] {relative_path}: {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ")
                        if keywords:
                            print(f"   í‚¤ì›Œë“œ: {', '.join(keywords[:5])}{'...' if len(keywords) > 5 else ''}")
                except Exception as e:
                    print(f"âŒ {md_file}: {e}")

    return file_keywords


def create_labels_from_keywords(keywords: Set[str], label_type: str = "keyword", db: Session = None) -> Dict[str, int]:
    """í‚¤ì›Œë“œë¡œë¶€í„° ë¼ë²¨ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ IDë§Œ ë°˜í™˜)"""
    keyword_to_label_id = {}

    for keyword in keywords:
        # ê¸°ì¡´ ë¼ë²¨ í™•ì¸
        existing = db.query(Label).filter(Label.name == keyword).first()

        if existing:
            keyword_to_label_id[keyword] = existing.id
        else:
            # ìƒˆ ë¼ë²¨ ìƒì„±
            new_label = Label(
                name=keyword,
                label_type=label_type,
                description=f"ë¬¸ì„œì—ì„œ ìë™ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keyword}"
            )
            db.add(new_label)
            db.commit()
            db.refresh(new_label)
            keyword_to_label_id[keyword] = new_label.id
            print(f"  ğŸ“Œ ë¼ë²¨ ìƒì„±: {keyword} (ID: {new_label.id})")

    return keyword_to_label_id


def auto_label_chunks(file_keywords: Dict[str, List[str]], db: Session):
    """ì²­í¬ì— í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ìë™ ë¼ë²¨ë§"""
    # ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘
    all_keywords = set()
    for keywords in file_keywords.values():
        all_keywords.update(keywords)

    # ë¼ë²¨ ìƒì„±
    keyword_to_label_id = create_labels_from_keywords(all_keywords, label_type="keyword", db=db)

    # ë¬¸ì„œë³„ë¡œ ì²˜ë¦¬
    for file_path, keywords in file_keywords.items():
        # í•´ë‹¹ íŒŒì¼ì˜ ë¬¸ì„œ ì°¾ê¸°
        document = db.query(Document).filter(Document.file_path == file_path).first()

        if not document:
            print(f"âš ï¸  ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {file_path}")
            continue

        # í•´ë‹¹ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ê°€ì ¸ì˜¤ê¸°
        chunks = db.query(KnowledgeChunk).filter(
            KnowledgeChunk.document_id == document.id
        ).all()

        labeled_count = 0
        for chunk in chunks:
            chunk_content_lower = chunk.content.lower()

            # ì²­í¬ ë‚´ìš©ì— í¬í•¨ëœ í‚¤ì›Œë“œ ì°¾ê¸°
            matched_keywords = [kw for kw in keywords if kw.lower() in chunk_content_lower]

            for keyword in matched_keywords:
                label_id = keyword_to_label_id[keyword]

                # ì´ë¯¸ ë¼ë²¨ì´ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                existing = db.query(KnowledgeLabel).filter(
                    KnowledgeLabel.chunk_id == chunk.id,
                    KnowledgeLabel.label_id == label_id
                ).first()

                if not existing:
                    # ìë™ ë¼ë²¨ë§ (source="ai", status="suggested")
                    knowledge_label = KnowledgeLabel(
                        chunk_id=chunk.id,
                        label_id=label_id,
                        confidence=0.7,  # í‚¤ì›Œë“œ ë§¤ì¹­ì´ë¯€ë¡œ ì¤‘ê°„ ì‹ ë¢°ë„
                        source="ai",
                        status="suggested"
                    )
                    db.add(knowledge_label)
                    labeled_count += 1

        if labeled_count > 0:
            db.commit()
            print(f"  âœ… {file_path}: {labeled_count}ê°œ ì²­í¬ì— ë¼ë²¨ ì—°ê²°")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìë™ ë¼ë²¨ë§ (docs ë° brain í´ë”)")
    parser.add_argument("--llm", action="store_true", help="LLMì„ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ë³¸ê°’: ì •ê·œì‹ ê¸°ë°˜)")
    parser.add_argument("--openai", action="store_true", help="OpenAI API ì‚¬ìš© (--llmê³¼ í•¨ê»˜ ì‚¬ìš©)")
    parser.add_argument("--docs-only", action="store_true", help="docs í´ë”ë§Œ ì²˜ë¦¬ (ê¸°ë³¸ê°’: docs + brain ëª¨ë‘ ì²˜ë¦¬)")
    args = parser.parse_args()

    print("=" * 60)
    print("í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìë™ ë¼ë²¨ë§")
    if args.llm:
        print("ëª¨ë“œ: LLM ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ")
        if args.openai:
            print("API: OpenAI")
        else:
            print("API: GPT4All (ë¡œì»¬)")
    else:
        print("ëª¨ë“œ: ì •ê·œì‹ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ")
    
    if args.docs_only:
        print("ëŒ€ìƒ: docs í´ë”ë§Œ")
    else:
        print("ëŒ€ìƒ: docs í´ë” + brain í´ë” (ê¸°ì¡´ íŒŒì¼ í¬í•¨)")
    print("=" * 60)

    # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
    print("\n[1/3] íŒŒì¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    file_keywords = process_docs_files(use_llm=args.llm, include_brain=not args.docs_only)

    if not file_keywords:
        print("ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. DB ì—°ê²°
    print("\n[2/3] ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
    db = SessionLocal()

    try:
        # 3. ë¼ë²¨ ìƒì„± ë° ìë™ ë¼ë²¨ë§
        print("\n[3/3] ë¼ë²¨ ìƒì„± ë° ì²­í¬ ìë™ ë¼ë²¨ë§ ì¤‘...")
        auto_label_chunks(file_keywords, db)

        print("\nâœ… ì™„ë£Œ!")
    finally:
        db.close()


if __name__ == "__main__":
    main()

